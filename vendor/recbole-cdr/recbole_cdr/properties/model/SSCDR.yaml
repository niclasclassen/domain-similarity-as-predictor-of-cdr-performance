embedding_size: 64
lambda: 0.25
margin: 1
mlp_hidden_size: [128]
train_epochs: ["SOURCE:100", "TARGET:100", "OVERLAP:100"]
overlap_batch_size: 100
