{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Statistical Test Analysis\n",
    "\n",
    "This notebook performs a two-tailed paired t-test to compare the performance metrics (HR, MRR, NDCG) between similar and dissimilar source domains. We use the performance metric results of overlapping users between two different source domains.\n",
    "\n",
    "## 1. Setup\n",
    "\n",
    "### 1.1 Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from scipy.stats import ttest_rel\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Perform two-tailed paired t-test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Approach 1 (Reproduction):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the parent directory containing all subfolders\n",
    "parent_directory = 'data/statistical_test'\n",
    "\n",
    "# Dynamically list all subfolders\n",
    "subfolders = [f.path for f in os.scandir(parent_directory) if f.is_dir()]\n",
    "\n",
    "# Define the filenames for the metrics\n",
    "metrics_files = {\n",
    "    'HR': 'user_hits.txt',\n",
    "    'MRR': 'user_mrrs.txt',\n",
    "    'NDCG': 'user_ndcg_at_10.txt',\n",
    "}\n",
    "\n",
    "# Define target and source domain pairs\n",
    "target_to_sources = {\n",
    "    'software': ['videogames', 'musicalinstruments'],\n",
    "    'musicalinstruments': ['videogames', 'software'],\n",
    "    'videogames': ['musicalinstruments', 'software'],\n",
    "}\n",
    "\n",
    "# Function to apply Benjamini-Hochberg correction\n",
    "def benjamini_hochberg_correction(p_values, alpha=0.05):\n",
    "    p_values = np.array(p_values)\n",
    "    n = len(p_values)\n",
    "    sorted_indices = np.argsort(p_values)\n",
    "    sorted_p_values = p_values[sorted_indices]\n",
    "    adjusted_p_values = np.empty(n)\n",
    "    for i in range(n):\n",
    "        adjusted_p_values[i] = sorted_p_values[i] * n / (i + 1)\n",
    "    adjusted_p_values = np.minimum.accumulate(adjusted_p_values[::-1])[::-1]\n",
    "    adjusted_p_values[adjusted_p_values > 1] = 1\n",
    "    return adjusted_p_values, sorted_indices\n",
    "\n",
    "# Initialize a list to store results\n",
    "results = []\n",
    "\n",
    "# Group folders by target domain and model\n",
    "grouped_folders = {}\n",
    "for folder in subfolders:\n",
    "    folder_name = os.path.basename(folder)\n",
    "    parts = folder_name.split('_')\n",
    "    if len(parts) < 4:\n",
    "        continue  # Skip invalid folder names\n",
    "    source_domain, target_domain, model = parts[2], parts[3], parts[4]\n",
    "\n",
    "    if target_domain not in grouped_folders:\n",
    "        grouped_folders[target_domain] = {}\n",
    "    if model not in grouped_folders[target_domain]:\n",
    "        grouped_folders[target_domain][model] = {}\n",
    "\n",
    "    grouped_folders[target_domain][model][source_domain] = folder\n",
    "\n",
    "# Iterate over target domains and models\n",
    "for target_domain, models in grouped_folders.items():\n",
    "    if target_domain not in target_to_sources:\n",
    "        continue\n",
    "\n",
    "    sources = target_to_sources[target_domain]\n",
    "    more_similar_source, less_similar_source = sources\n",
    "\n",
    "    for model, source_folders in models.items():\n",
    "        # Ensure both source folders are available\n",
    "        if more_similar_source not in source_folders or less_similar_source not in source_folders:\n",
    "            continue\n",
    "\n",
    "        similar_folder = source_folders[more_similar_source]\n",
    "        dissimilar_folder = source_folders[less_similar_source]\n",
    "\n",
    "        # Paths to user_id_mapping\n",
    "        file_user_id_mapping = 'user_id_mapping.txt'\n",
    "        file_path_similar = os.path.join(similar_folder, file_user_id_mapping)\n",
    "        file_path_dissimilar = os.path.join(dissimilar_folder, file_user_id_mapping)\n",
    "\n",
    "        # Read the user_id_mapping files\n",
    "        df_similar = pd.read_csv(file_path_similar, header=None, names=['new_id', 'original_id']).drop_duplicates()\n",
    "        df_dissimilar = pd.read_csv(file_path_dissimilar, header=None, names=['new_id', 'original_id']).drop_duplicates()\n",
    "\n",
    "        # Filter for overlapping original_ids\n",
    "        df_similar_filtered = df_similar[df_similar['original_id'].isin(df_dissimilar['original_id'])]\n",
    "        df_dissimilar_filtered = df_dissimilar[df_dissimilar['original_id'].isin(df_similar['original_id'])]\n",
    "\n",
    "        # Read metric files and perform paired t-tests\n",
    "        for metric, file_name in metrics_files.items():\n",
    "            file_path_metric_similar = os.path.join(similar_folder, file_name)\n",
    "            file_path_metric_dissimilar = os.path.join(dissimilar_folder, file_name)\n",
    "\n",
    "            # Read metric data\n",
    "            df_metric_similar = pd.read_csv(file_path_metric_similar, header=None, names=[metric.lower()])\n",
    "            df_metric_dissimilar = pd.read_csv(file_path_metric_dissimilar, header=None, names=[metric.lower()])\n",
    "\n",
    "            # Add new_id column based on index\n",
    "            df_metric_similar['new_id'] = df_metric_similar.index + 1\n",
    "            df_metric_dissimilar['new_id'] = df_metric_dissimilar.index + 1\n",
    "\n",
    "            # Filter for overlapping new_ids\n",
    "            df_metric_similar_filtered = df_metric_similar[df_metric_similar['new_id'].isin(df_similar_filtered['new_id'])]\n",
    "            df_metric_dissimilar_filtered = df_metric_dissimilar[df_metric_dissimilar['new_id'].isin(df_dissimilar_filtered['new_id'])]\n",
    "\n",
    "            # Ensure alignment by user_id\n",
    "            df_metric_similar_filtered = df_metric_similar_filtered.merge(df_similar_filtered, on='new_id', how='left').set_index('original_id')\n",
    "            df_metric_dissimilar_filtered = df_metric_dissimilar_filtered.merge(df_dissimilar_filtered, on='new_id', how='left').set_index('original_id')\n",
    "\n",
    "            # Perform paired t-test\n",
    "            t_stat, p_value = ttest_rel(\n",
    "                df_metric_similar_filtered[metric.lower()],\n",
    "                df_metric_dissimilar_filtered[metric.lower()]\n",
    "            )\n",
    "            results.append({\n",
    "                'model': model,\n",
    "                'target': target_domain,\n",
    "                'metric': metric,\n",
    "                'more_similar_source': more_similar_source,\n",
    "                'less_similar_source': less_similar_source,\n",
    "                't_statistic': t_stat,\n",
    "                'p_value': p_value\n",
    "            })\n",
    "\n",
    "# Convert results to a DataFrame\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "# Apply Benjamini-Hochberg correction\n",
    "p_values = results_df['p_value'].values\n",
    "adjusted_p_values, sorted_indices = benjamini_hochberg_correction(p_values)\n",
    "\n",
    "# Add adjusted p-values to the DataFrame\n",
    "results_df['adjusted_p_value'] = adjusted_p_values[np.argsort(sorted_indices)]\n",
    "\n",
    "# Save the results to a CSV file\n",
    "results_df.to_csv('paired_t_test_results_with_bh.csv', index=False)\n",
    "\n",
    "# Display the results\n",
    "print(results_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Approach 2 (Semantic Similarity):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Full splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define the parent directory containing all subfolders\n",
    "parent_directory = 'data/statistical_test/own_research/full_splits'\n",
    "\n",
    "# Dynamically list all subfolders\n",
    "subfolders = [f.path for f in os.scandir(parent_directory) if f.is_dir()]\n",
    "\n",
    "# Define the filenames for the metrics\n",
    "metrics_files = {\n",
    "    'HR': 'user_hits.txt',\n",
    "    'MRR': 'user_mrrs.txt',\n",
    "    'NDCG': 'user_ndcg_at_10.txt',\n",
    "}\n",
    "\n",
    "# Define target and source domain pairs\n",
    "target_to_sources = [\n",
    "    ('books', ['electronics', 'house']),\n",
    "    ('books', ['movies', 'electronics']),\n",
    "    ('books', ['movies', 'house']),\n",
    "]\n",
    "\n",
    "# Function to apply Benjamini-Hochberg correction\n",
    "def benjamini_hochberg_correction(p_values, alpha=0.05):\n",
    "    p_values = np.array(p_values)\n",
    "    n = len(p_values)\n",
    "    sorted_indices = np.argsort(p_values)\n",
    "    sorted_p_values = p_values[sorted_indices]\n",
    "    adjusted_p_values = np.empty(n)\n",
    "    for i in range(n):\n",
    "        adjusted_p_values[i] = sorted_p_values[i] * n / (i + 1)\n",
    "    adjusted_p_values = np.minimum.accumulate(adjusted_p_values[::-1])[::-1]\n",
    "    adjusted_p_values[adjusted_p_values > 1] = 1\n",
    "    return adjusted_p_values, sorted_indices\n",
    "\n",
    "# Initialize a list to store results\n",
    "results = []\n",
    "\n",
    "# Group folders by target domain and model\n",
    "grouped_folders = {}\n",
    "for folder in subfolders:\n",
    "    folder_name = os.path.basename(folder)\n",
    "    parts = folder_name.split('_')\n",
    "    if len(parts) < 4:\n",
    "        continue  # Skip invalid folder names\n",
    "    source_domain, target_domain, model = parts[3], parts[2], parts[4]\n",
    "\n",
    "    if target_domain not in grouped_folders:\n",
    "        grouped_folders[target_domain] = {}\n",
    "    if model not in grouped_folders[target_domain]:\n",
    "        grouped_folders[target_domain][model] = {}\n",
    "\n",
    "    grouped_folders[target_domain][model][source_domain] = folder\n",
    "\n",
    "# Iterate over each target-source pair combination\n",
    "for target_domain, sources in target_to_sources:\n",
    "    if target_domain not in grouped_folders:\n",
    "        continue\n",
    "\n",
    "    more_similar_source, less_similar_source = sources\n",
    "\n",
    "    models = grouped_folders.get(target_domain, {})\n",
    "    for model, source_folders in models.items():\n",
    "        # Ensure both source folders are available\n",
    "        if more_similar_source not in source_folders or less_similar_source not in source_folders:\n",
    "            print(f\"Skipping model {model} for target {target_domain} due to missing sources.\")\n",
    "            continue\n",
    "\n",
    "        similar_folder = source_folders[more_similar_source]\n",
    "        dissimilar_folder = source_folders[less_similar_source]\n",
    "\n",
    "        # Paths to user_id_mapping\n",
    "        file_user_id_mapping = 'user_id_mapping.txt'\n",
    "        file_path_similar = os.path.join(similar_folder, file_user_id_mapping)\n",
    "        file_path_dissimilar = os.path.join(dissimilar_folder, file_user_id_mapping)\n",
    "\n",
    "        # Read the user_id_mapping files\n",
    "        df_similar = pd.read_csv(file_path_similar, header=None, names=['new_id', 'original_id']).drop_duplicates()\n",
    "        df_dissimilar = pd.read_csv(file_path_dissimilar, header=None, names=['new_id', 'original_id']).drop_duplicates()\n",
    "\n",
    "        # Filter for overlapping original_ids\n",
    "        df_similar_filtered = df_similar[df_similar['original_id'].isin(df_dissimilar['original_id'])]\n",
    "        df_dissimilar_filtered = df_dissimilar[df_dissimilar['original_id'].isin(df_similar['original_id'])]\n",
    "\n",
    "        # Read metric files and perform paired t-tests\n",
    "        for metric, file_name in metrics_files.items():\n",
    "            file_path_metric_similar = os.path.join(similar_folder, file_name)\n",
    "            file_path_metric_dissimilar = os.path.join(dissimilar_folder, file_name)\n",
    "\n",
    "            # Read metric data\n",
    "            df_metric_similar = pd.read_csv(file_path_metric_similar, header=None, names=[metric.lower()])\n",
    "            df_metric_dissimilar = pd.read_csv(file_path_metric_dissimilar, header=None, names=[metric.lower()])\n",
    "\n",
    "            # Add new_id column based on index\n",
    "            df_metric_similar['new_id'] = df_metric_similar.index + 1\n",
    "            df_metric_dissimilar['new_id'] = df_metric_dissimilar.index + 1\n",
    "\n",
    "            # Filter for overlapping new_ids\n",
    "            df_metric_similar_filtered = df_metric_similar[df_metric_similar['new_id'].isin(df_similar_filtered['new_id'])]\n",
    "            df_metric_dissimilar_filtered = df_metric_dissimilar[df_metric_dissimilar['new_id'].isin(df_dissimilar_filtered['new_id'])]\n",
    "\n",
    "            # Ensure alignment by user_id\n",
    "            df_metric_similar_filtered = df_metric_similar_filtered.merge(df_similar_filtered, on='new_id', how='left').set_index('original_id')\n",
    "            df_metric_dissimilar_filtered = df_metric_dissimilar_filtered.merge(df_dissimilar_filtered, on='new_id', how='left').set_index('original_id')\n",
    "\n",
    "            # Perform paired t-test\n",
    "            t_stat, p_value = ttest_rel(\n",
    "                df_metric_similar_filtered[metric.lower()],\n",
    "                df_metric_dissimilar_filtered[metric.lower()]\n",
    "            )\n",
    "            results.append({\n",
    "                'model': model,\n",
    "                'target': target_domain,\n",
    "                'metric': metric,\n",
    "                'more_similar_source': more_similar_source,\n",
    "                'less_similar_source': less_similar_source,\n",
    "                't_statistic': t_stat,\n",
    "                'p_value': p_value\n",
    "            })\n",
    "\n",
    "# Convert results to a DataFrame\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "# Apply Benjamini-Hochberg correction\n",
    "p_values = results_df['p_value'].values\n",
    "adjusted_p_values, sorted_indices = benjamini_hochberg_correction(p_values)\n",
    "\n",
    "# Add adjusted p-values to the DataFrame\n",
    "results_df['adjusted_p_value'] = adjusted_p_values[np.argsort(sorted_indices)]\n",
    "\n",
    "# Save the results to a CSV file\n",
    "results_df.to_csv('paired_t_test_results_with_bh.csv', index=False)\n",
    "\n",
    "# Display the results\n",
    "print(results_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Top/Bottom Splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the parent directory containing all subfolders\n",
    "parent_directory = 'data/statistical_test/own_research/top_bottom'\n",
    "\n",
    "# Dynamically list all subfolders\n",
    "subfolders = [f.path for f in os.scandir(parent_directory) if f.is_dir()]\n",
    "\n",
    "# Define the filenames for the metrics\n",
    "metrics_files = {\n",
    "    'HR': 'user_hits.txt',\n",
    "    'MRR': 'user_mrrs.txt',\n",
    "    'NDCG': 'user_ndcg_at_10.txt',\n",
    "}\n",
    "\n",
    "# Define target and source domain pairs\n",
    "target_to_sources = [\n",
    "    ('books', ['elt30', 'elb30']),\n",
    "    ('books', ['hot30', 'hob30']),\n",
    "    ('books', ['mot30', 'mob30']),\n",
    "]\n",
    "\n",
    "# Function to apply Benjamini-Hochberg correction\n",
    "def benjamini_hochberg_correction(p_values, alpha=0.05):\n",
    "    p_values = np.array(p_values)\n",
    "    n = len(p_values)\n",
    "    sorted_indices = np.argsort(p_values)\n",
    "    sorted_p_values = p_values[sorted_indices]\n",
    "    adjusted_p_values = np.empty(n)\n",
    "    for i in range(n):\n",
    "        adjusted_p_values[i] = sorted_p_values[i] * n / (i + 1)\n",
    "    adjusted_p_values = np.minimum.accumulate(adjusted_p_values[::-1])[::-1]\n",
    "    adjusted_p_values[adjusted_p_values > 1] = 1\n",
    "    return adjusted_p_values, sorted_indices\n",
    "\n",
    "# Initialize a list to store results\n",
    "results = []\n",
    "\n",
    "# Group folders by target domain and model\n",
    "grouped_folders = {}\n",
    "for folder in subfolders:\n",
    "    folder_name = os.path.basename(folder)\n",
    "    parts = folder_name.split('_')\n",
    "    if len(parts) < 4:\n",
    "        continue  # Skip invalid folder names\n",
    "    source_domain, target_domain, model = parts[3], parts[2], parts[4]\n",
    "\n",
    "    if target_domain not in grouped_folders:\n",
    "        grouped_folders[target_domain] = {}\n",
    "    if model not in grouped_folders[target_domain]:\n",
    "        grouped_folders[target_domain][model] = {}\n",
    "\n",
    "    grouped_folders[target_domain][model][source_domain] = folder\n",
    "\n",
    "# Iterate over each target-source pair combination\n",
    "for target_domain, sources in target_to_sources:\n",
    "    if target_domain not in grouped_folders:\n",
    "        continue\n",
    "\n",
    "    more_similar_source, less_similar_source = sources\n",
    "\n",
    "    models = grouped_folders.get(target_domain, {})\n",
    "    for model, source_folders in models.items():\n",
    "        # Ensure both source folders are available\n",
    "        if more_similar_source not in source_folders or less_similar_source not in source_folders:\n",
    "            print(f\"Skipping model {model} for target {target_domain} due to missing sources.\")\n",
    "            continue\n",
    "\n",
    "        similar_folder = source_folders[more_similar_source]\n",
    "        dissimilar_folder = source_folders[less_similar_source]\n",
    "\n",
    "        # Paths to user_id_mapping\n",
    "        file_user_id_mapping = 'user_id_mapping.txt'\n",
    "        file_path_similar = os.path.join(similar_folder, file_user_id_mapping)\n",
    "        file_path_dissimilar = os.path.join(dissimilar_folder, file_user_id_mapping)\n",
    "\n",
    "        # Read the user_id_mapping files\n",
    "        df_similar = pd.read_csv(file_path_similar, header=None, names=['new_id', 'original_id']).drop_duplicates()\n",
    "        df_dissimilar = pd.read_csv(file_path_dissimilar, header=None, names=['new_id', 'original_id']).drop_duplicates()\n",
    "\n",
    "        # Filter for overlapping original_ids\n",
    "        df_similar_filtered = df_similar[df_similar['original_id'].isin(df_dissimilar['original_id'])]\n",
    "        df_dissimilar_filtered = df_dissimilar[df_dissimilar['original_id'].isin(df_similar['original_id'])]\n",
    "\n",
    "        # Read metric files and perform paired t-tests\n",
    "        for metric, file_name in metrics_files.items():\n",
    "            file_path_metric_similar = os.path.join(similar_folder, file_name)\n",
    "            file_path_metric_dissimilar = os.path.join(dissimilar_folder, file_name)\n",
    "\n",
    "            # Read metric data\n",
    "            df_metric_similar = pd.read_csv(file_path_metric_similar, header=None, names=[metric.lower()])\n",
    "            df_metric_dissimilar = pd.read_csv(file_path_metric_dissimilar, header=None, names=[metric.lower()])\n",
    "\n",
    "            # Add new_id column based on index\n",
    "            df_metric_similar['new_id'] = df_metric_similar.index + 1\n",
    "            df_metric_dissimilar['new_id'] = df_metric_dissimilar.index + 1\n",
    "\n",
    "            # Filter for overlapping new_ids\n",
    "            df_metric_similar_filtered = df_metric_similar[df_metric_similar['new_id'].isin(df_similar_filtered['new_id'])]\n",
    "            df_metric_dissimilar_filtered = df_metric_dissimilar[df_metric_dissimilar['new_id'].isin(df_dissimilar_filtered['new_id'])]\n",
    "\n",
    "            # Ensure alignment by user_id\n",
    "            df_metric_similar_filtered = df_metric_similar_filtered.merge(df_similar_filtered, on='new_id', how='left').set_index('original_id')\n",
    "            df_metric_dissimilar_filtered = df_metric_dissimilar_filtered.merge(df_dissimilar_filtered, on='new_id', how='left').set_index('original_id')\n",
    "\n",
    "            # Perform paired t-test\n",
    "            t_stat, p_value = ttest_rel(\n",
    "                df_metric_similar_filtered[metric.lower()],\n",
    "                df_metric_dissimilar_filtered[metric.lower()]\n",
    "            )\n",
    "            results.append({\n",
    "                'model': model,\n",
    "                'target': target_domain,\n",
    "                'metric': metric,\n",
    "                'more_similar_source': more_similar_source,\n",
    "                'less_similar_source': less_similar_source,\n",
    "                't_statistic': t_stat,\n",
    "                'p_value': p_value\n",
    "            })\n",
    "\n",
    "# Convert results to a DataFrame\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "# Apply Benjamini-Hochberg correction\n",
    "p_values = results_df['p_value'].values\n",
    "adjusted_p_values, sorted_indices = benjamini_hochberg_correction(p_values)\n",
    "\n",
    "# Add adjusted p-values to the DataFrame\n",
    "results_df['adjusted_p_value'] = adjusted_p_values[np.argsort(sorted_indices)]\n",
    "\n",
    "# Save the results to a CSV file\n",
    "results_df.to_csv('paired_t_test_results_with_bh.csv', index=False)\n",
    "\n",
    "# Display the results\n",
    "print(results_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Approach 3 (Pattern Similarity):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the parent directory containing all subfolders\n",
    "parent_directory = 'data/statistical_test/own_research/pattern_similarity'\n",
    "\n",
    "# Dynamically list all subfolders\n",
    "subfolders = [f.path for f in os.scandir(parent_directory) if f.is_dir()]\n",
    "\n",
    "# Define the filenames for the metrics\n",
    "metrics_files = {\n",
    "    'HR': 'user_hits.txt',\n",
    "    'MRR': 'user_mrrs.txt',\n",
    "    'NDCG': 'user_ndcg_at_10.txt',\n",
    "}\n",
    "\n",
    "# Define target and source domain pairs\n",
    "target_to_sources = [\n",
    "    ('books', ['elt25', 'elb25']),\n",
    "    ('books', ['hot25', 'hob25']),\n",
    "    ('books', ['mot25', 'mob25']),\n",
    "]\n",
    "\n",
    "# Function to apply Benjamini-Hochberg correction\n",
    "def benjamini_hochberg_correction(p_values, alpha=0.05):\n",
    "    p_values = np.array(p_values)\n",
    "    n = len(p_values)\n",
    "    sorted_indices = np.argsort(p_values)\n",
    "    sorted_p_values = p_values[sorted_indices]\n",
    "    adjusted_p_values = np.empty(n)\n",
    "    for i in range(n):\n",
    "        adjusted_p_values[i] = sorted_p_values[i] * n / (i + 1)\n",
    "    adjusted_p_values = np.minimum.accumulate(adjusted_p_values[::-1])[::-1]\n",
    "    adjusted_p_values[adjusted_p_values > 1] = 1\n",
    "    return adjusted_p_values, sorted_indices\n",
    "\n",
    "# Initialize a list to store results\n",
    "results = []\n",
    "\n",
    "# Group folders by target domain and model\n",
    "grouped_folders = {}\n",
    "for folder in subfolders:\n",
    "    folder_name = os.path.basename(folder)\n",
    "    parts = folder_name.split('_')\n",
    "    if len(parts) < 4:\n",
    "        continue  # Skip invalid folder names\n",
    "    source_domain, target_domain, model = parts[3], parts[2], parts[4]\n",
    "\n",
    "    if target_domain not in grouped_folders:\n",
    "        grouped_folders[target_domain] = {}\n",
    "    if model not in grouped_folders[target_domain]:\n",
    "        grouped_folders[target_domain][model] = {}\n",
    "\n",
    "    grouped_folders[target_domain][model][source_domain] = folder\n",
    "\n",
    "# Iterate over each target-source pair combination\n",
    "for target_domain, sources in target_to_sources:\n",
    "    if target_domain not in grouped_folders:\n",
    "        continue\n",
    "\n",
    "    more_similar_source, less_similar_source = sources\n",
    "\n",
    "    models = grouped_folders.get(target_domain, {})\n",
    "    for model, source_folders in models.items():\n",
    "        # Ensure both source folders are available\n",
    "        if more_similar_source not in source_folders or less_similar_source not in source_folders:\n",
    "            print(f\"Skipping model {model} for target {target_domain} due to missing sources.\")\n",
    "            continue\n",
    "\n",
    "        similar_folder = source_folders[more_similar_source]\n",
    "        dissimilar_folder = source_folders[less_similar_source]\n",
    "\n",
    "        # Paths to user_id_mapping\n",
    "        file_user_id_mapping = 'user_id_mapping.txt'\n",
    "        file_path_similar = os.path.join(similar_folder, file_user_id_mapping)\n",
    "        file_path_dissimilar = os.path.join(dissimilar_folder, file_user_id_mapping)\n",
    "\n",
    "        # Read the user_id_mapping files\n",
    "        df_similar = pd.read_csv(file_path_similar, header=None, names=['new_id', 'original_id']).drop_duplicates()\n",
    "        df_dissimilar = pd.read_csv(file_path_dissimilar, header=None, names=['new_id', 'original_id']).drop_duplicates()\n",
    "\n",
    "        # Filter for overlapping original_ids\n",
    "        df_similar_filtered = df_similar[df_similar['original_id'].isin(df_dissimilar['original_id'])]\n",
    "        df_dissimilar_filtered = df_dissimilar[df_dissimilar['original_id'].isin(df_similar['original_id'])]\n",
    "\n",
    "        # Read metric files and perform paired t-tests\n",
    "        for metric, file_name in metrics_files.items():\n",
    "            file_path_metric_similar = os.path.join(similar_folder, file_name)\n",
    "            file_path_metric_dissimilar = os.path.join(dissimilar_folder, file_name)\n",
    "\n",
    "            # Read metric data\n",
    "            df_metric_similar = pd.read_csv(file_path_metric_similar, header=None, names=[metric.lower()])\n",
    "            df_metric_dissimilar = pd.read_csv(file_path_metric_dissimilar, header=None, names=[metric.lower()])\n",
    "\n",
    "            # Add new_id column based on index\n",
    "            df_metric_similar['new_id'] = df_metric_similar.index + 1\n",
    "            df_metric_dissimilar['new_id'] = df_metric_dissimilar.index + 1\n",
    "\n",
    "            # Filter for overlapping new_ids\n",
    "            df_metric_similar_filtered = df_metric_similar[df_metric_similar['new_id'].isin(df_similar_filtered['new_id'])]\n",
    "            df_metric_dissimilar_filtered = df_metric_dissimilar[df_metric_dissimilar['new_id'].isin(df_dissimilar_filtered['new_id'])]\n",
    "\n",
    "            # Ensure alignment by user_id\n",
    "            df_metric_similar_filtered = df_metric_similar_filtered.merge(df_similar_filtered, on='new_id', how='left').set_index('original_id')\n",
    "            df_metric_dissimilar_filtered = df_metric_dissimilar_filtered.merge(df_dissimilar_filtered, on='new_id', how='left').set_index('original_id')\n",
    "\n",
    "            # Perform paired t-test\n",
    "            t_stat, p_value = ttest_rel(\n",
    "                df_metric_similar_filtered[metric.lower()],\n",
    "                df_metric_dissimilar_filtered[metric.lower()]\n",
    "            )\n",
    "            results.append({\n",
    "                'model': model,\n",
    "                'target': target_domain,\n",
    "                'metric': metric,\n",
    "                'more_similar_source': more_similar_source,\n",
    "                'less_similar_source': less_similar_source,\n",
    "                't_statistic': t_stat,\n",
    "                'p_value': p_value\n",
    "            })\n",
    "\n",
    "# Convert results to a DataFrame\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "# Apply Benjamini-Hochberg correction\n",
    "p_values = results_df['p_value'].values\n",
    "adjusted_p_values, sorted_indices = benjamini_hochberg_correction(p_values)\n",
    "\n",
    "# Add adjusted p-values to the DataFrame\n",
    "results_df['adjusted_p_value'] = adjusted_p_values[np.argsort(sorted_indices)]\n",
    "\n",
    "# Save the results to a CSV file\n",
    "results_df.to_csv('paired_t_test_results_with_bh.csv', index=False)\n",
    "\n",
    "# Display the results\n",
    "print(results_df)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
